{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 15px; height: 80px\">\n",
    "\n",
    "# Project 5\n",
    "\n",
    "\n",
    "## NLP and Machine Learning on [travel.statsexchange.com](http://travel.stackexchange.com/) data\n",
    "\n",
    "---\n",
    "\n",
    "In Project 7 you'll be doing NLP and machine learning on post data from stackexchange's travel subdomain. \n",
    "\n",
    "This project is setup like a mini Kaggle competition. You are given the training data and when projects are submitted your model will be tested on the held-out testing data. There will be prizes for the people who build models that perform best on the held out test set!\n",
    "\n",
    "---\n",
    "\n",
    "## Notes on the data\n",
    "\n",
    "The data is again compressed into the `.7z` file format to save space. There are 6 .csv files and one readme file that contains some information on the fields.\n",
    "\n",
    "    posts_train.csv\n",
    "    comments_train.csv\n",
    "    users.csv\n",
    "    badges.csv\n",
    "    votes_train.csv\n",
    "    tags.csv\n",
    "    readme.txt\n",
    "    \n",
    "The data is located in your datasets folder:\n",
    "\n",
    "    DSI-SF-2/datasets/stack_exchange_travel.7z\n",
    "    \n",
    "If you're interested in where this data came from and where to get more data from other stackexchange subdomains, see here:\n",
    "\n",
    "https://ia800500.us.archive.org/22/items/stackexchange/readme.txt\n",
    "\n",
    "\n",
    "### Recommended Utilities for .7z\n",
    "\n",
    "- For OSX [Keka](http://www.kekaosx.com/en/) or [The Unarchiver](http://wakaba.c3.cx/s/apps/unarchiver.html). \n",
    "- For Windows [7-zip](http://www.7-zip.org/) is the standard. \n",
    "- For Linux try the `p7zip` utility.  `sudo apt-get install p7zip`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns \n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords # Import the stop word list\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "from gensim import corpora, models\n",
    "import gensim \n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import auc, roc_curve\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "from scipy.stats.mstats import gmean\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src=\"http://imgur.com/l5NasQj.png\" style=\"float: left; margin: 25px 15px 0px 0px; height: 25px\">\n",
    "\n",
    "### 1. Use LDA to find what topics are discussed on travel.stackexchange.com.\n",
    "\n",
    "---\n",
    "\n",
    "Text can be found in the posts and the comments datasets. The `ParentId` column in the posts dataset indicates what the \"question\" post was for a given post. Comment text can be merged onto the post they are part of with the `PostId` field.\n",
    "\n",
    "The text may have some HTML tags. BeautifulSoup has convenient ways to get rid of markup or extract text if you need to. You can also parse the strings yourself if you like.\n",
    "\n",
    "The tags dataset has the \"tags\" that the users have officially given the post.\n",
    "\n",
    "**1.1 Implement LDA against the text features of the dataset(s).**\n",
    "\n",
    "- This can be posts or a combination of posts and comments if you want more power.\n",
    "- Find optimal **K/num_topics**.\n",
    "\n",
    "**1.2 Compare your topics to the tags. Do the LDA topics make sense? How do they compare to the tags?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "post = pd.read_csv('../../datasets/stack_exchange_travel/posts_train.csv')\n",
    "comment = pd.read_csv('../../datasets/stack_exchange_travel/comments_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AcceptedAnswerId</th>\n",
       "      <th>AnswerCount</th>\n",
       "      <th>Body</th>\n",
       "      <th>ClosedDate</th>\n",
       "      <th>CommentCount</th>\n",
       "      <th>CommunityOwnedDate</th>\n",
       "      <th>CreationDate</th>\n",
       "      <th>FavoriteCount</th>\n",
       "      <th>Id</th>\n",
       "      <th>LastActivityDate</th>\n",
       "      <th>...</th>\n",
       "      <th>LastEditorDisplayName</th>\n",
       "      <th>LastEditorUserId</th>\n",
       "      <th>OwnerDisplayName</th>\n",
       "      <th>OwnerUserId</th>\n",
       "      <th>ParentId</th>\n",
       "      <th>PostTypeId</th>\n",
       "      <th>Score</th>\n",
       "      <th>Tags</th>\n",
       "      <th>Title</th>\n",
       "      <th>ViewCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>393.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>&lt;p&gt;My fiancée and I are looking for a good Car...</td>\n",
       "      <td>2013-02-25T23:52:47.953</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2011-06-21T20:19:34.730</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2012-05-24T14:52:14.760</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>101.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>&lt;caribbean&gt;&lt;cruising&gt;&lt;vacations&gt;</td>\n",
       "      <td>What are some Caribbean cruises for October?</td>\n",
       "      <td>361.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>&lt;p&gt;Singapore Airlines has an all-business clas...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2011-06-21T20:24:57.160</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>2013-01-09T09:55:22.743</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>693.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>24.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>&lt;loyalty-programs&gt;&lt;routes&gt;&lt;ewr&gt;&lt;singapore-airl...</td>\n",
       "      <td>Does Singapore Airlines offer any reward seats...</td>\n",
       "      <td>219.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>770.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>&lt;p&gt;Another definition question that interested...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2011-06-21T20:25:56.787</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5</td>\n",
       "      <td>2012-10-12T20:49:08.110</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>101.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>&lt;romania&gt;&lt;transportation&gt;</td>\n",
       "      <td>What is the easiest transportation to use thro...</td>\n",
       "      <td>340.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   AcceptedAnswerId  AnswerCount  \\\n",
       "0             393.0          4.0   \n",
       "1               NaN          1.0   \n",
       "2             770.0          5.0   \n",
       "\n",
       "                                                Body               ClosedDate  \\\n",
       "0  <p>My fiancée and I are looking for a good Car...  2013-02-25T23:52:47.953   \n",
       "1  <p>Singapore Airlines has an all-business clas...                      NaN   \n",
       "2  <p>Another definition question that interested...                      NaN   \n",
       "\n",
       "   CommentCount CommunityOwnedDate             CreationDate  FavoriteCount  \\\n",
       "0             4                NaN  2011-06-21T20:19:34.730            NaN   \n",
       "1             1                NaN  2011-06-21T20:24:57.160            NaN   \n",
       "2             0                NaN  2011-06-21T20:25:56.787            2.0   \n",
       "\n",
       "   Id         LastActivityDate    ...    LastEditorDisplayName  \\\n",
       "0   1  2012-05-24T14:52:14.760    ...                      NaN   \n",
       "1   4  2013-01-09T09:55:22.743    ...                      NaN   \n",
       "2   5  2012-10-12T20:49:08.110    ...                      NaN   \n",
       "\n",
       "  LastEditorUserId  OwnerDisplayName OwnerUserId  ParentId  PostTypeId  Score  \\\n",
       "0            101.0               NaN         9.0       NaN           1      8   \n",
       "1            693.0               NaN        24.0       NaN           1      8   \n",
       "2            101.0               NaN        13.0       NaN           1     11   \n",
       "\n",
       "                                                Tags  \\\n",
       "0                   <caribbean><cruising><vacations>   \n",
       "1  <loyalty-programs><routes><ewr><singapore-airl...   \n",
       "2                          <romania><transportation>   \n",
       "\n",
       "                                               Title ViewCount  \n",
       "0       What are some Caribbean cruises for October?     361.0  \n",
       "1  Does Singapore Airlines offer any reward seats...     219.0  \n",
       "2  What is the easiest transportation to use thro...     340.0  \n",
       "\n",
       "[3 rows x 21 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# mask = (post['ParentId'] == 1) & (post['PostTypeId']==2)\n",
    "# temp_df = pd.DataFrame(post[mask]) # create new temporary dataframe to hold question and answers\n",
    "# pd.concat(objs=[post[post['Id']==1], temp_df], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# mask = (post['PostTypeId']==2) & (post['ParentId']==1)\n",
    "# # post.iloc[0]\n",
    "# pd.concat(objs=[post.iloc[0],post[mask]], axis=1, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "post.dropna(axis=0, subset=['Body'], inplace=True)\n",
    "post = post.reset_index()\n",
    "post = post.drop(labels='index',axis=1)\n",
    "post = post[['Id'] + post.columns.tolist()[:8] + post.columns.tolist()[9:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def processHTML(text, stemming=True, output_type='string'):\n",
    "    text = BeautifulSoup(text).get_text()\n",
    "\n",
    "    text = text.replace('\\n', '')\n",
    "        \n",
    "    # 2. Extract special negator like n't\n",
    "    text = re.sub('n\\'t', ' not', text)\n",
    "    \n",
    "    letters_only = re.sub('[^a-zA-Z]',\n",
    "                     ' ',\n",
    "                     text)\n",
    "    \n",
    "    words = letters_only.lower().split()       # Convert to lower case\n",
    "    \n",
    "    stop = set(stopwords.words('english'))\n",
    "    \n",
    "    meaningful_words = [w for w in words if w not in stop]\n",
    "    \n",
    "    # Optional stemming \n",
    "    if stemming == True:\n",
    "        s_stemmer = SnowballStemmer('english')\n",
    "        meaningful_words = [s_stemmer.stem(w) for w in meaningful_words]\n",
    "    \n",
    "    if output_type == 'string':\n",
    "        return(\" \".join(meaningful_words)) \n",
    "    elif output_type == 'list':\n",
    "        return meaningful_words\n",
    "\n",
    "    \n",
    "def getCleanPosts(df, output_type='string'):\n",
    "    clean_posts = []\n",
    "    for i, post in enumerate(df['Body'].values):        \n",
    "        if (i+1) % 5000 == 0:\n",
    "            print 'cleaning review #{} out of {}'.format(i+1, df.shape[0])\n",
    "        clean_posts.append(processHTML(post, output_type))\n",
    "    \n",
    "    return clean_posts\n",
    "        \n",
    "def getBOW(posts, ngram_range=(1,3), max_features=5000, min_df=0.0, max_df = 1.0):\n",
    "    # First create the vectorizer class\n",
    "    vectorizer = CountVectorizer(ngram_range = ngram_range,\n",
    "                                 min_df = min_df,\n",
    "                                 max_df = max_df,\n",
    "                                 analyzer='word',\n",
    "                                 max_features = max_features,\n",
    "                                 stop_words=None,\n",
    "                                 preprocessor=None,\n",
    "                                 tokenizer=None\n",
    "                                )\n",
    "\n",
    "    feature_matrix = vectorizer.fit_transform(posts)\n",
    "\n",
    "    return feature_matrix  \n",
    "\n",
    "def getTFIDF( clean_reviews, ngram_range=(1,3), max_features=5000, min_df=0.0, max_df = 1.0):\n",
    "    vectorizer = TfidfVectorizer(ngram_range = ngram_range,\n",
    "                                 min_df = min_df,\n",
    "                                 max_df = max_df,\n",
    "                                 analyzer='word',\n",
    "                                 max_features = max_features,\n",
    "                                 stop_words=None,\n",
    "                                 preprocessor=None,\n",
    "                                 tokenizer=None\n",
    "                                )\n",
    "    \n",
    "\n",
    "    feature_matrix = vectorizer.fit_transform(clean_reviews)\n",
    "\n",
    "    return feature_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Naekid/anaconda3/envs/dsi/lib/python2.7/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 174 of the file /Users/Naekid/anaconda3/envs/dsi/lib/python2.7/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaning review #5000 out of 40476\n",
      "cleaning review #10000 out of 40476\n",
      "cleaning review #15000 out of 40476\n",
      "cleaning review #20000 out of 40476\n",
      "cleaning review #25000 out of 40476\n",
      "cleaning review #30000 out of 40476\n",
      "cleaning review #35000 out of 40476\n",
      "cleaning review #40000 out of 40476\n"
     ]
    }
   ],
   "source": [
    "posts = getCleanPosts(post)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "** Convert to Bag of Words **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# feature_matrix = getBOW(posts, ngram_range=(1,1))\n",
    "\n",
    "# feature_matrix = feature_matrix.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(posts)\n",
    "corpus = [dictionary.doc2bow(text) for text in posts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# https://radimrehurek.com/gensim/models/ldamulticore.html#module-gensim.models.ldamulticore\n",
    "ldamulticore = gensim.models.ldamulticore.LdaMulticore(corpus, num_topics=100, workers=3, id2word=dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(41, u'0.009*\"http\" + 0.009*\"strike\" + 0.009*\"travel\" + 0.008*\"visa\" + 0.008*\"would\"'), (98, u'0.011*\"restaur\" + 0.010*\"find\" + 0.008*\"bar\" + 0.007*\"like\" + 0.007*\"place\"'), (61, u'0.015*\"us\" + 0.014*\"passport\" + 0.010*\"record\" + 0.009*\"uk\" + 0.008*\"travel\"'), (75, u'0.018*\"batteri\" + 0.011*\"pakistan\" + 0.010*\"airport\" + 0.008*\"surnam\" + 0.006*\"travel\"'), (64, u'0.014*\"camp\" + 0.011*\"sleep\" + 0.008*\"also\" + 0.007*\"take\" + 0.006*\"car\"'), (51, u'0.028*\"card\" + 0.022*\"bank\" + 0.009*\"v\" + 0.008*\"statement\" + 0.008*\"use\"'), (83, u'0.029*\"ticket\" + 0.027*\"flight\" + 0.024*\"book\" + 0.017*\"airlin\" + 0.011*\"check\"'), (42, u'0.027*\"visa\" + 0.017*\"countri\" + 0.016*\"travel\" + 0.015*\"schengen\" + 0.011*\"would\"'), (91, u'0.016*\"seat\" + 0.012*\"use\" + 0.011*\"would\" + 0.011*\"travel\" + 0.008*\"flight\"'), (6, u'0.022*\"marri\" + 0.016*\"certif\" + 0.014*\"birth\" + 0.012*\"marriag\" + 0.010*\"travel\"')]\n"
     ]
    }
   ],
   "source": [
    "print(ldamulticore.print_topics(num_topics=10, num_words=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80, 0.96586206896551619)\n"
     ]
    }
   ],
   "source": [
    "print(ldamulticore[corpus[0]])[0] # get topic probability distribution for a document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'0.012*\"place\" + 0.012*\"one\" + 0.010*\"like\" + 0.009*\"go\" + 0.008*\"would\" + 0.007*\"time\" + 0.007*\"get\" + 0.006*\"food\" + 0.006*\"look\" + 0.006*\"travel\" + 0.005*\"cruis\" + 0.005*\"day\" + 0.005*\"car\" + 0.005*\"restaur\" + 0.005*\"also\" + 0.005*\"take\" + 0.004*\"good\" + 0.004*\"way\" + 0.004*\"even\" + 0.004*\"buy\"'"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_num = 0\n",
    "ldamulticore.print_topic(ldamulticore[corpus[doc_num]][0][0], 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<p>My fianc\\xc3\\xa9e and I are looking for a good Caribbean cruise in October and were wondering which islands are best to see and which Cruise line to take?</p>\\n\\n<p>It seems like a lot of the cruises don't run in this month due to Hurricane season so I'm looking for other good options.</p>\\n\\n<p><strong>EDIT</strong> We'll be travelling in 2012.</p>\\n\""
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post['Body'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The word list for the topic of the first document ('place', 'one','like') doesn't really match up with the tags for that document ('caribbean','cruising','vacation')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.026*\"flight\" + 0.016*\"mile\" + 0.013*\"airlin\" + 0.009*\"ticket\" + 0.008*\"get\" + 0.007*\"time\" + 0.007*\"fare\" + 0.007*\"one\" + 0.006*\"fli\" + 0.006*\"need\" + 0.006*\"would\" + 0.005*\"book\" + 0.005*\"also\" + 0.005*\"may\" + 0.005*\"program\" + 0.004*\"check\" + 0.004*\"frequent\" + 0.004*\"free\" + 0.004*\"travel\" + 0.004*\"aa\" \n",
      "\n",
      "<p>Singapore Airlines has an all-business class flight from EWR-SIN (Newark->Singapore), but I can't seem to find any reward Krisflyer flights for <em>any</em> dates.  </p>\n",
      "\n",
      "<loyalty-programs><routes><ewr><singapore-airlines><sin>\n"
     ]
    }
   ],
   "source": [
    "doc_num = 1\n",
    "print ldamulticore.print_topic(ldamulticore[corpus[doc_num]][0][0], 20), '\\n'\n",
    "print post['Body'][doc_num]\n",
    "print post['Tags'][doc_num]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The 'Body' text matches more closely with the words in the topic ('flight','mile','airline','ticket'), which sort of match the tags. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**NOTE** I'm not sure how to find the optimal number of topics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src=\"http://imgur.com/l5NasQj.png\" style=\"float: left; margin: 25px 15px 0px 0px; height: 25px\">\n",
    "\n",
    "### 2. What makes an answer likely to be \"accepted\"?\n",
    "\n",
    "---\n",
    "\n",
    "**2.1 Build a model to predict whether a post will be marked as the answer.**\n",
    "\n",
    "- This is a classification problem.\n",
    "- You're free to use any of the machine learning algorithms or techniques we have learned in class to build the best model you can.\n",
    "- NLP will be very useful here for pulling out useful and relevant features from the data. \n",
    "- Though not required, using bagging and boosting models like Random Forests and Gradient Boosted Trees will _probably_ get you the highest performance on the test data (but who knows!).\n",
    "\n",
    "\n",
    "**2.2 Evaluate the performance of your classifier with a confusion matrix and accuracy. Explain how your model is performing.**\n",
    "\n",
    "**2.3 Plot either a ROC curve or precision-recall curve (or both!) and explain what they tell you about your model.**\n",
    "\n",
    "NOTE: You should only be predicting this for `PostTypeID=2` posts, which are the \"answer\" posts. This doesn't mean, however, that you can't or shouldn't use the parent questions as predictors!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "I'm thinking that maybe you can quantify how similar an answer is to a question, and then use that info to make the prediction. Similarity can be quantified using cosine similarity. And in order to vectorize the text we can use tf-idf. I'm thinking that we should:\n",
    "1. vectorize the text by making each set of question and answer it's own corpus (actually let's just first try using the entire dataset for our corpus). \n",
    "2. Then vectorize each corpus. \n",
    "3. Then calculate cosine similarity between each question-answer pair. \n",
    "4. Also calculate the length of each answer.\n",
    "4. Create a feature for each answer from it's cosine similarity, and a feature from the answer's length. \n",
    "5. Run Random Forest, XGBoost, Logistic Regression using the cosine similarity as the feature. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaning review #5000 out of 40476\n",
      "cleaning review #10000 out of 40476\n",
      "cleaning review #15000 out of 40476\n",
      "cleaning review #20000 out of 40476\n",
      "cleaning review #25000 out of 40476\n",
      "cleaning review #30000 out of 40476\n",
      "cleaning review #35000 out of 40476\n",
      "cleaning review #40000 out of 40476\n"
     ]
    }
   ],
   "source": [
    "# We need to make a dataframe that contains only questions and answers \n",
    "posts_strings = getCleanPosts(post, output_type='string')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Insert column for cleaned post text within original dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "body_cleaned = pd.Series(posts_strings, name='Body_cleaned')\n",
    "\n",
    "post.insert(loc=4, column='Body_cleaned', value=body_cleaned.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "** TF-IDF on question-answer groups (with PCA) **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "qa_groups = post.copy()\n",
    "qa_groups['AcceptedAnswer'] = 0\n",
    "qa_groups['component1'] = 0\n",
    "qa_groups['component2'] = 0\n",
    "qa_groups['component3'] = 0\n",
    "qa_groups['component4'] = 0\n",
    "# qa_groups.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating cosine similarity for row 0 of 40476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Naekid/anaconda3/envs/dsi/lib/python2.7/site-packages/sklearn/decomposition/pca.py:398: RuntimeWarning: invalid value encountered in divide\n",
      "  explained_variance_ratio_ = explained_variance_ / total_var\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating cosine similarity for row 500 of 40476\n",
      "calculating cosine similarity for row 1000 of 40476\n",
      "calculating cosine similarity for row 1500 of 40476\n",
      "calculating cosine similarity for row 2000 of 40476\n",
      "calculating cosine similarity for row 2500 of 40476\n",
      "calculating cosine similarity for row 3000 of 40476\n",
      "calculating cosine similarity for row 3500 of 40476\n",
      "calculating cosine similarity for row 4000 of 40476\n",
      "calculating cosine similarity for row 4500 of 40476\n",
      "calculating cosine similarity for row 5000 of 40476\n",
      "calculating cosine similarity for row 5500 of 40476\n",
      "calculating cosine similarity for row 6000 of 40476\n",
      "calculating cosine similarity for row 6500 of 40476\n",
      "calculating cosine similarity for row 7000 of 40476\n",
      "calculating cosine similarity for row 7500 of 40476\n",
      "calculating cosine similarity for row 8000 of 40476\n",
      "calculating cosine similarity for row 8500 of 40476\n",
      "calculating cosine similarity for row 9000 of 40476\n",
      "calculating cosine similarity for row 9500 of 40476\n",
      "calculating cosine similarity for row 10000 of 40476\n",
      "calculating cosine similarity for row 10500 of 40476\n",
      "calculating cosine similarity for row 11000 of 40476\n",
      "calculating cosine similarity for row 11500 of 40476\n",
      "calculating cosine similarity for row 12000 of 40476\n",
      "calculating cosine similarity for row 12500 of 40476\n",
      "calculating cosine similarity for row 13000 of 40476\n",
      "calculating cosine similarity for row 13500 of 40476\n",
      "calculating cosine similarity for row 14000 of 40476\n",
      "calculating cosine similarity for row 14500 of 40476\n",
      "calculating cosine similarity for row 15000 of 40476\n",
      "calculating cosine similarity for row 15500 of 40476\n",
      "calculating cosine similarity for row 16000 of 40476\n",
      "calculating cosine similarity for row 16500 of 40476\n",
      "calculating cosine similarity for row 17000 of 40476\n",
      "calculating cosine similarity for row 17500 of 40476\n",
      "calculating cosine similarity for row 18000 of 40476\n",
      "calculating cosine similarity for row 18500 of 40476\n",
      "calculating cosine similarity for row 19000 of 40476\n",
      "calculating cosine similarity for row 19500 of 40476\n",
      "calculating cosine similarity for row 20000 of 40476\n",
      "calculating cosine similarity for row 20500 of 40476\n",
      "calculating cosine similarity for row 21000 of 40476\n",
      "calculating cosine similarity for row 21500 of 40476\n",
      "calculating cosine similarity for row 22000 of 40476\n",
      "calculating cosine similarity for row 22500 of 40476\n",
      "calculating cosine similarity for row 23000 of 40476\n",
      "calculating cosine similarity for row 23500 of 40476\n",
      "calculating cosine similarity for row 24000 of 40476\n",
      "calculating cosine similarity for row 24500 of 40476\n",
      "calculating cosine similarity for row 25000 of 40476\n",
      "calculating cosine similarity for row 25500 of 40476\n",
      "calculating cosine similarity for row 26000 of 40476\n",
      "calculating cosine similarity for row 26500 of 40476\n",
      "calculating cosine similarity for row 27000 of 40476\n",
      "calculating cosine similarity for row 27500 of 40476\n",
      "calculating cosine similarity for row 28000 of 40476\n",
      "calculating cosine similarity for row 28500 of 40476\n",
      "calculating cosine similarity for row 29000 of 40476\n",
      "calculating cosine similarity for row 29500 of 40476\n",
      "calculating cosine similarity for row 30000 of 40476\n",
      "calculating cosine similarity for row 30500 of 40476\n",
      "calculating cosine similarity for row 31000 of 40476\n",
      "calculating cosine similarity for row 31500 of 40476\n",
      "calculating cosine similarity for row 32000 of 40476\n",
      "calculating cosine similarity for row 32500 of 40476\n",
      "calculating cosine similarity for row 33000 of 40476\n",
      "calculating cosine similarity for row 33500 of 40476\n",
      "calculating cosine similarity for row 34000 of 40476\n",
      "calculating cosine similarity for row 34500 of 40476\n",
      "calculating cosine similarity for row 35000 of 40476\n",
      "calculating cosine similarity for row 35500 of 40476\n",
      "calculating cosine similarity for row 36000 of 40476\n",
      "calculating cosine similarity for row 36500 of 40476\n",
      "calculating cosine similarity for row 37000 of 40476\n",
      "calculating cosine similarity for row 37500 of 40476\n",
      "calculating cosine similarity for row 38000 of 40476\n",
      "calculating cosine similarity for row 38500 of 40476\n",
      "calculating cosine similarity for row 39000 of 40476\n",
      "calculating cosine similarity for row 39500 of 40476\n",
      "calculating cosine similarity for row 40000 of 40476\n"
     ]
    }
   ],
   "source": [
    "for i, index in enumerate(post.index.values):\n",
    "    if i % 500 == 0:\n",
    "        print \"calculating tf-idf for row {} of {}\".format(i, post.index.values.shape[0])\n",
    "\n",
    "    if post.ix[index,'PostTypeId'] == 1: # Question\n",
    "        \n",
    "        post_id = post.ix[index,'Id'] # grab question post ID\n",
    "\n",
    "        accepted_answer_id = post.ix[index,'AcceptedAnswerId'] # grab the answer ID\n",
    "        try:\n",
    "            accepted_answer_id = int(accepted_answer_id)\n",
    "        except: # the accepted_answer_id is np.nan\n",
    "            accepted_answer_id = accepted_answer_id\n",
    "\n",
    "        mask = (post['ParentId'] == post_id) & (post['PostTypeId']==2) # create mask to find answers to this question\n",
    "\n",
    "        # create new temporary dataframe to hold question and answers\n",
    "        temp_df = pd.DataFrame(post[mask]) \n",
    "        temp_df = pd.concat(objs=[post[post['Id']==post_id], temp_df], axis=0)\n",
    "\n",
    "        # create array of post text \n",
    "        text_array = temp_df['Body_cleaned'].values\n",
    "\n",
    "        # use tf-idf on new temp dataframe \n",
    "        tfidf_matrix = getTFIDF(text_array, ngram_range=(1,3)).todense()\n",
    "\n",
    "        # Reduce size using PCA \n",
    "        n_components = 4 # CHOOSE THIS \n",
    "        \n",
    "        pca = PCA(n_components=n_components)\n",
    "        pca_matrix = pca.fit_transform(tfidf_matrix)\n",
    "\n",
    "        # If pca reduced our features below n_components, then we won't be able to concat \n",
    "        if pca_matrix.shape[1] < n_components: \n",
    "            rows = pca_matrix.shape[0]\n",
    "            cols = n_components-pca_matrix.shape[1]\n",
    "            b = np.zeros((rows, n_components)) # make a matrix that is the size of pca_matrix + however many cols needed to make n_components\n",
    "            b[:,:-cols] = pca_matrix\n",
    "            pca_matrix = b\n",
    "        \n",
    "\n",
    "         # create new feature/column in qa_groups in temp_df with cosine similarity value for each answer \n",
    "        tfidf_df = pd.DataFrame(pca_matrix, index=temp_df.index.values, columns=['component1', 'component2', 'component3', 'component4'])\n",
    "\n",
    "        # Concatenate old dataframe with new \n",
    "        temp_df = pd.concat(objs=[temp_df, tfidf_df], axis=1)\n",
    "\n",
    "         # Assign values in CosineSimilarity column in temp_df to the respective rows in the qa_groups df \n",
    "        for row in temp_df.index.values:\n",
    "\n",
    "            # update new dataframe with cosine similarity values\n",
    "            qa_groups.ix[row, 'component1'] = temp_df.ix[row, 'component1']\n",
    "            qa_groups.ix[row, 'component2'] = temp_df.ix[row, 'component2']\n",
    "            qa_groups.ix[row, 'component3'] = temp_df.ix[row, 'component3']\n",
    "            qa_groups.ix[row, 'component4'] = temp_df.ix[row, 'component4']\n",
    "\n",
    "            # update new dataframe with AcceptedAnswer status\n",
    "            if qa_groups.ix[row, 'Id'] == accepted_answer_id:\n",
    "                qa_groups.ix[row, 'AcceptedAnswer'] = 1\n",
    "            else:\n",
    "                qa_groups.ix[row, 'AcceptedAnswer'] = 0\n",
    "                \n",
    "qa_groups['PostLength'] = qa_groups['Body_cleaned'].apply(lambda x: len(x))                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "answers = qa_groups[qa_groups['PostTypeId']==2]  # grab just the answers\n",
    "answers = answers.dropna(axis=0, subset=['AcceptedAnswer']) # drop the rows with np.nan in 'AcceptedAnswer's\n",
    "answers['AcceptedAnswer'] = answers['AcceptedAnswer'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create design matrix \n",
    "features = ['CommentCount','component1', 'component2', 'component3', 'component4']\n",
    "X = answers[features].values # grab features \n",
    "y = answers['AcceptedAnswer'].values # grab target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.728126181096\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "pred = lr.predict(X_test)\n",
    "\n",
    "scores = cross_val_score(estimator=lr, X=X, y=y, cv=7, scoring='accuracy')\n",
    "\n",
    "print np.mean(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.696582801352\n"
     ]
    }
   ],
   "source": [
    "rfc = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "scores = cross_val_score(estimator=rfc, X=X, y=y, cv=3, scoring='accuracy')\n",
    "\n",
    "print np.mean(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "** TF-IDF + BoW + features => PCA => modeling (using entire corpus) **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating answer column for row 0 of 40476\n",
      "creating answer column for row 5000 of 40476\n",
      "creating answer column for row 10000 of 40476\n",
      "creating answer column for row 15000 of 40476\n",
      "creating answer column for row 20000 of 40476\n",
      "creating answer column for row 25000 of 40476\n",
      "creating answer column for row 30000 of 40476\n",
      "creating answer column for row 35000 of 40476\n",
      "creating answer column for row 40000 of 40476\n"
     ]
    }
   ],
   "source": [
    "qa_all = post.copy()\n",
    "qa_all['AcceptedAnswer'] = 0\n",
    "\n",
    "for i, index in enumerate(post.index.values):\n",
    "    if i % 5000 == 0:\n",
    "        print \"creating answer column for row {} of {}\".format(i, post.index.values.shape[0])\n",
    "\n",
    "    if post.ix[index,'PostTypeId'] == 1: # Question\n",
    "        \n",
    "        post_id = post.ix[index,'Id'] # grab question post ID\n",
    "\n",
    "        accepted_answer_id = post.ix[index,'AcceptedAnswerId'] # grab the answer ID\n",
    "        try:\n",
    "            accepted_answer_id = int(accepted_answer_id)\n",
    "        except: # the accepted_answer_id is np.nan\n",
    "            accepted_answer_id = accepted_answer_id\n",
    "\n",
    "        mask = (post['ParentId'] == post_id) & (post['PostTypeId']==2) # create mask to find answers to this question\n",
    "\n",
    "        # create new temporary dataframe to hold question and answers\n",
    "        temp_df = pd.DataFrame(post[mask]) \n",
    "        temp_df = pd.concat(objs=[post[post['Id']==post_id], temp_df], axis=0)\n",
    "\n",
    "        for row in temp_df.index.values:\n",
    "\n",
    "            # update new dataframe with AcceptedAnswer status\n",
    "            if qa_all.ix[row, 'Id'] == accepted_answer_id:\n",
    "                qa_all.ix[row, 'AcceptedAnswer'] = 1\n",
    "            else:\n",
    "                qa_all.ix[row, 'AcceptedAnswer'] = 0\n",
    "                \n",
    "qa_all['PostLength'] = qa_all['Body_cleaned'].apply(lambda x: len(x))                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "answers = qa_all[qa_all['PostTypeId']==2]  # grab just the answers\n",
    "answers = answers.dropna(axis=0, subset=['AcceptedAnswer']) # drop the rows with np.nan in 'AcceptedAnswer's\n",
    "answers['AcceptedAnswer'] = answers['AcceptedAnswer'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaning review #5000 out of 23967\n",
      "cleaning review #10000 out of 23967\n",
      "cleaning review #15000 out of 23967\n",
      "cleaning review #20000 out of 23967\n"
     ]
    }
   ],
   "source": [
    "clean_answers = getCleanPosts(answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "answers = answers[['ViewCount','Score','FavoriteCount','CommentCount','AcceptedAnswer']]\n",
    "answers = answers.drop(labels=['ViewCount','FavoriteCount','Score'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating TF-IDF matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=5)\n",
    "\n",
    "vec = TfidfVectorizer(ngram_range = (1,3),\n",
    "                     analyzer='word',\n",
    "                     max_features = 3000,\n",
    "                     stop_words=None,\n",
    "                     preprocessor=None,\n",
    "                     tokenizer=None\n",
    "                    )\n",
    "\n",
    "tfidf_matrix = vec.fit_transform(clean_answers).todense()\n",
    "\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating BoW Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "countvec = CountVectorizer(ngram_range = (1,3),\n",
    "                             analyzer='word',\n",
    "                             max_features=3000,\n",
    "                             stop_words=None,\n",
    "                             preprocessor=None,\n",
    "                             tokenizer=None\n",
    "                            )\n",
    "\n",
    "bow_matrix = countvec.fit_transform(clean_answers).todense()\n",
    "\n",
    "bow_df = pd.DataFrame(bow_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transforming with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "bow_pca = pca.fit_transform(bow_matrix)\n",
    "tfidf_pca = pca.fit_transform(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Converting matrices into dataframes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "bow_pca_df = pd.DataFrame(bow_pca)\n",
    "tfidf_pca_df = pd.DataFrame(tfidf_pca)\n",
    "final_df = pd.concat(objs=[bow_pca_df, tfidf_pca_df, answers], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create design matrix \n",
    "X = answers.ix[:,:-1].values # grab features \n",
    "y = answers['AcceptedAnswer'].values # grab target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.727875829265\n"
     ]
    }
   ],
   "source": [
    "rfc = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "scores = cross_val_score(estimator=rfc, X=X, y=y, cv=3, scoring='accuracy')\n",
    "\n",
    "print np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "** Cosine Similarity **\n",
    "----------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#---- TESTS -----#\n",
    "# mask = (post['ParentId'] == 1) & (post['PostTypeId']==2)\n",
    "# temp_df = pd.DataFrame(post[mask]) # create new temporary dataframe to hold question and answers\n",
    "\n",
    "# temp_df = pd.concat(objs=[post[post['Id']==1], temp_df], axis=0)\n",
    "\n",
    "# text_array = temp_df['Body_cleaned'].values\n",
    "\n",
    "# tfidf_matrix = getTFIDF(text_array, max_features=20, ngram_range=(1,3)).todense()\n",
    "\n",
    "# cosine_similarity = (tfidf_matrix * tfidf_matrix.T).A\n",
    "\n",
    "# tfidf_df = pd.DataFrame(cosine_similarity[:, 0], index=temp_df.index.values, columns=['CosineSimilarity'])\n",
    "\n",
    "# pd.concat(objs=[temp_df, tfidf_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "question_answers = post.copy()\n",
    "question_answers['CosineSimilarity'] = np.nan\n",
    "question_answers['AcceptedAnswer'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating cosine similarity for row 0 of 40476\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n"
     ]
    }
   ],
   "source": [
    "# I hope there are no questions with 0 answers in the post df\n",
    "\n",
    "for i, index in enumerate(post.index.values[:500]):\n",
    "    if i % 500 == 0:\n",
    "        print \"calculating cosine similarity for row {} of {}\".format(i, post.index.values.shape[0])\n",
    "\n",
    "    if post.ix[index,'PostTypeId'] == 1: # Question\n",
    "        \n",
    "        post_id = post.ix[index,'Id'] # grab question post ID\n",
    "        \n",
    "        accepted_answer_id = post.ix[index,'AcceptedAnswerId'] # grab the answer ID\n",
    "        try:\n",
    "            accepted_answer_id = int(accepted_answer_id)\n",
    "        except: # the accepted_answer_id is np.nan\n",
    "            accepted_answer_id = accepted_answer_id\n",
    "            \n",
    "        mask = (post['ParentId'] == post_id) & (post['PostTypeId']==2) # create mask to find answers to this question\n",
    "        \n",
    "        # create new temporary dataframe to hold question and answers\n",
    "        temp_df = pd.DataFrame(post[mask]) \n",
    "        temp_df = pd.concat(objs=[post[post['Id']==post_id], temp_df], axis=0)\n",
    "        \n",
    "        # create array of post text \n",
    "        text_array = temp_df['Body_cleaned'].values\n",
    "        \n",
    "        # use tf-idf on new temp dataframe \n",
    "        tfidf_matrix = getTFIDF(text_array, max_features=20, ngram_range=(1,3)).todense()\n",
    "        \n",
    "        # calculate cosine similarity between each question (first row) and each answer (other rows)\n",
    "        cosine_similarity = (tfidf_matrix * tfidf_matrix.T).A\n",
    "\n",
    "        # create new feature/column in question_answers in temp_df with cosine similarity value for each answer \n",
    "        tfidf_df = pd.DataFrame(cosine_similarity[:, 0], index=temp_df.index.values, columns=['CosineSimilarity'])\n",
    "        \n",
    "        # Concatenate old dataframe with new \n",
    "        temp_df = pd.concat(objs=[temp_df, tfidf_df], axis=1)\n",
    "        \n",
    "        # Assign values in CosineSimilarity column in temp_df to the respective rows in the question_answers df \n",
    "        for row in temp_df.index.values:\n",
    "            \n",
    "            # update new dataframe with cosine similarity values\n",
    "            question_answers.ix[row, 'CosineSimilarity'] = temp_df.ix[row, 'CosineSimilarity']\n",
    "                   \n",
    "            # update new dataframe with AcceptedAnswer status\n",
    "            if question_answers.ix[row, 'Id'] == accepted_answer_id:\n",
    "                question_answers.ix[row, 'AcceptedAnswer'] = 1\n",
    "            else:\n",
    "                question_answers.ix[row, 'AcceptedAnswer'] = 0\n",
    "            \n",
    "            \n",
    "        del temp_df #delete dataframe\n",
    "        \n",
    "question_answers['PostLength'] = question_answers['Body_cleaned'].apply(lambda x: len(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "6. Run RF, XGBOOST, LR to see which model performs best. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# grab just the answers\n",
    "answers = question_answers[question_answers['PostTypeId']==2] \n",
    "# drop the rows with np.nan in 'AcceptedAnswer's\n",
    "answers = answers.dropna(axis=0, subset=['AcceptedAnswer'])\n",
    "answers['AcceptedAnswer'] = answers['AcceptedAnswer'].astype(int)\n",
    "# answers[answers['AcceptedAnswer'].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Check the baseline and imbalance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline accuracy: 0.728126173489\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division \n",
    "\n",
    "print 'baseline accuracy:', 1-answers[answers['AcceptedAnswer']==1].shape[0] / answers.shape[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.726707440186\n"
     ]
    }
   ],
   "source": [
    "# Create design matrix \n",
    "features = ['PostLength']\n",
    "X = answers[features].values # grab features \n",
    "y = answers['AcceptedAnswer'].values # grab target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
    "\n",
    "lr = LogisticRegression()\n",
    "\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "pred = lr.predict(X_test)\n",
    "\n",
    "scores = cross_val_score(estimator=lr, X=X, y=y, cv=7, scoring='accuracy')\n",
    "\n",
    "print np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.72679087259\n"
     ]
    }
   ],
   "source": [
    "# Create design matrix \n",
    "features = ['CosineSimilarity','PostLength']\n",
    "X = answers[features].values # grab features \n",
    "y = answers['AcceptedAnswer'].values # grab target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
    "\n",
    "lr = LogisticRegression()\n",
    "\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "pred = lr.predict(X_test)\n",
    "\n",
    "scores = cross_val_score(estimator=lr, X=X, y=y, cv=7, scoring='accuracy')\n",
    "\n",
    "print np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  72 out of  72 | elapsed:  4.8min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
       "            verbose=0, warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'n_estimators': [100, 400], 'min_samples_split': [4, 7], 'max_depth': [None, 50, 100], 'min_samples_leaf': [2, 6]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='accuracy', verbose=1)"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {\n",
    "    'n_estimators':[100, 400],\n",
    "    'max_depth':[None,50, 100], \n",
    "    'min_samples_split':[4, 7], \n",
    "    'min_samples_leaf':[2, 6]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(estimator=RandomForestClassifier(), param_grid=params, scoring='accuracy', verbose=1, cv=3)\n",
    "\n",
    "gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.710843373494\n"
     ]
    }
   ],
   "source": [
    "print gs.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---\n",
    "** Conclusion**: I tried the following methods:\n",
    "1. TF-IDF on question answer pairs + PCA -> LR, RF  -> accuracy : 0.728126181096\n",
    "2. TF-IDF on question answer pairs + Cosine Similarity -> LR, RF -> accuracy: 0.72679087259\n",
    "3. TF-IDF + BoW + PCA on entire corpus -> LR, RF -> accuracy: 0.727875829265\n",
    "\n",
    "But I was unable to get an accuracy score that was higher than the baseline (72%). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src=\"http://imgur.com/l5NasQj.png\" style=\"float: left; margin: 25px 15px 0px 0px; height: 25px\">\n",
    "\n",
    "### 3. What is the score of a post?\n",
    "\n",
    "---\n",
    "\n",
    "**3.1 Build a model that predicts the score of a post.**\n",
    "\n",
    "- This is a regression problem now. \n",
    "- You can and should be predicting score for both \"question\" and \"answer\" posts, so keep them both in your dataset.\n",
    "- Again, use any techniques that you think will get you the best model.\n",
    "\n",
    "**3.2 Evaluate the performance of your model with cross-validation and report the results.**\n",
    "\n",
    "**3.3 What is important for determining the score of a post, if anything?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src=\"http://imgur.com/l5NasQj.png\" style=\"float: left; margin: 25px 15px 0px 0px; height: 25px\">\n",
    "\n",
    "### 4. How many views does a post have?\n",
    "\n",
    "---\n",
    "\n",
    "**4.1 Build a model that predicts the number of views a post has.**\n",
    "\n",
    "- This is another regression problem. \n",
    "- Predict the views for all posts, not just the \"answer\" posts.\n",
    "\n",
    "**4.2 Evaluate the performance of your model with cross-validation and report the results.**\n",
    "\n",
    "**4.3 What is important for the number of views a post has, if anything?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src=\"http://imgur.com/l5NasQj.png\" style=\"float: left; margin: 25px 15px 0px 0px; height: 25px\">\n",
    "\n",
    "### 5. Build a pipeline or other code to automate evaluation of your models on the test data.\n",
    "\n",
    "---\n",
    "\n",
    "Now that you've constructed your three predictive models, build a pipeline or code that can easily load up the raw testing data and evaluate your models on it.\n",
    "\n",
    "The testing data that is held out is in the same raw format as the training data you have. _Any cleaning and preprocessing that you did on the training data will need to be done on the testing data as well!_\n",
    "\n",
    "This is a good opportunity to practice building pipelines, but you're not required to. Custom functions and classes are fine as long as they are able to process and test the new data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src=\"http://imgur.com/xDpSobf.png\" style=\"float: left; margin: 25px 15px 0px 0px; height: 25px\">\n",
    "\n",
    "## 6. Lets Model - Tournament for stock market predictions\n",
    "\n",
    ">Start this section of the project by downloading the train and test datasets from the following site: https://numer.ai/rules\n",
    "\n",
    "> - The data set is clean, your goal is to develop a classification model(s) \n",
    "> - Report all the results including log loss, and other coefficients you consider iteresting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
